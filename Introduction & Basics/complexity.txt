THESE ARE THE NOTES ON CHAPTER 2 ON COMPLEXITY FOR COMPUTER SCIENCE DISTILLED.

How long does it take to sort 26 shuffled cards? or whatever amount of cards? the time taken is equivalent to the method used in sorting the cards.
A method that requires a finite series of operations is called an ALGORITHM.
Less operations means less computing power and in the end we want fast solutions so we monitior the number of operations our algorithms have, Many algorithms require fast growing number of operations when the input size increases, so we find the TIME COMPLEXITY to avoid bad surprises.

TIME COMPLEXITY T(n): This basically describes the number of operations an algorithm performs when processing an input of size n. It is also described as the running cost.

Input size is not the only characteristic that affects the number of operations needed by the algorithm. When an algorithm can have different value of T(n) for the same value of n, we resort to cases.

1. BEST CASE: When the input requires the minimum number of operations for any input of that size.
2. WORST CASE: When the input requires the maximum number of operations for any input of that size.
3. AVERAGE CASE: When the input requires the average number of operations for typical inputs of the said size.

In general the most important is the worst case, because it gives you a baseline you can always count on.

COUNTING TIME
we find time complexity by counting the number of basic operations it requires for a hypothetical input n.

Understanding Growth: T(n) can be approximated by using the fastest growing term called the DOMINANT TERM. This is basically used to calculate how execution time will grow. the basic idea is the dominant term is the largest term in the calculation for the processes in the algorithm.
If we plot a graph to compare complexities especially those with the same dominant term we'll see that reagrdless of the the coefficient, the larger the value grows the closer the curves come together until they finally are the same or are on the same line.
Remember, this effect of curves getting closer works if the fastest growing term is the same. The plot of a function with a linear growth (n) never gets closer and closer to one with a quadratic growth (n2), which in turn never gets closer and closer to one having a cubic growth (n3).

The big O notation is just a fancy way of representing the dominant term in the expressing the rate of growth of execution with respect to the change in input values.

Exponential times are the worst time complexities avaliable and are considered not runnable. The same could be said for quadratic times but it doesn't have the same ridiculous time taken as the exponential time. The worst time complexity to consider is the factorial time. 

COUNTING MEMORY
The calculation memory is called the space complexity basically refers to the amount of computer memory need to run the operations to completion within the said time complexity.

guidelines for asymptotic analysis
1. Loops : this is the running time of statements inside the loop multiplied by the number of iterations e.g

#executing times
for i in range(0, n):
    print('current number': i) # constant time
Total time is constant time c multiplied by the number of iterations(n) = c * n

2. Netsed loops: O(n2)
3. Consecutive statements: well this can be a combination of nested loops, a single statement a single loop etc.
4. if then- else statement have O(n)

the idea of size of input is in the case of an array the numer of items in the array but in the case of say, adding two numbers, the size of the input will be the number of bits necessary to express this in ordinary binary notation.

the average case running time of an algo is almost the same as the worst case, what it does is half the amount of iterations but it does not change the amount of instructions needed to run the algo to completion.
